{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando Dataset e DataFrame(leitura de CSV) - Aula 5\n",
    "## construindo dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cod: long (nullable = true)\n",
      " |-- nome: string (nullable = true)\n",
      "\n",
      "None\n",
      "+---+----------+\n",
      "|cod|      nome|\n",
      "+---+----------+\n",
      "|  1| 'Rodrigo'|\n",
      "|  2|'Reboucas'|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#criando dataframe e salvando como CSV para usar nesta e proxima aulas\n",
    "#importando bibliotecas do sql e Row\n",
    "from pyspark.sql import Row\n",
    "#Criando lista para usar como arquivo CSV, primeiro utilizando Name e depois salvando como Dataframe\n",
    "Name = Row(\"cod\",\"nome\")\n",
    "data = [Name(1,\"'Rodrigo'\"),Name(2,\"'Reboucas'\")]\n",
    "#Criando um dataframe como a atribuição de Data\n",
    "data_frame = spark.createDataFrame(data)\n",
    "#Imprimindo o schema criando do \"data_frame\" e Visualizando todas as linhas com um \".show\" *em bancos grandes sempre limitar as linhas\n",
    "print(data_frame.printSchema())\n",
    "data_frame.show()\n",
    "#salvando o \"data_frame\" na pasta do hdfs com caminho \"/user...\", sobreescrevendo com \"overwrite\" e lendo o cabeçalho com \"header=true\"\n",
    "data_frame.write.csv(\"/user/ronnan/teste_csv\", mode=\"overwrite\", header=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Leitura de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|cod|    nome|\n",
      "+---+--------+\n",
      "|  2|Reboucas|\n",
      "|  1| Rodrigo|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lendo o arquivo csv e lendo o cabeçalho com \"header=True\"\n",
    "spark.read.csv(\"/user/ronnan/teste_csv\",header=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|    cod,nome|\n",
      "+------------+\n",
      "|2,'Reboucas'|\n",
      "| 1,'Rodrigo'|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lendo o arquivo csv e lendo o cabeçalho com \"header=True\" e lendo com \";\"\n",
    "spark.read.csv(\"/user/ronnan/teste_csv\",header=True,sep = \";\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|    cod,nome|\n",
      "+------------+\n",
      "|2,'Reboucas'|\n",
      "| 1,'Rodrigo'|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lendo o arquivo csv e lendo o cabeçalho com \"header=True\", lendo com \";\" e tirando espaços com \"ignoreLeadingWhiteSpace\"\n",
    "spark.read.csv(\"/user/ronnan/teste_csv\",header=True,sep = \";\", ignoreLeadingWhiteSpace=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|cod|    nome|\n",
      "+---+--------+\n",
      "|  2|Reboucas|\n",
      "|  1| Rodrigo|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lendo o arquivo csv e lendo o cabeçalho com \"header=True\", tirando espaços com \"ignoreLeadingWhiteSpace\" e tirando os aspas simples('') com quote=\"\\''\"\n",
    "spark.read.csv(\"/user/ronnan/teste_csv\",header=True, ignoreLeadingWhiteSpace=True, quote=\"\\'\",).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\" TRADUZIR ESSE TEXTO E LER COM CALMA\n",
    "Signature: spark.read.csv(path, schema=None, sep=None, encoding=None, quote=None, escape=None, comment=None, header=None, inferSchema=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, positiveInf=None, negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None, maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None, columnNameOfCorruptRecord=None, multiLine=None, charToEscapeQuoteEscaping=None, samplingRatio=None, enforceSchema=None, emptyValue=None)\n",
    "Docstring:\n",
    "Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
    "\n",
    "This function will go through the input once to determine the input schema if\n",
    "``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
    "``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
    "\n",
    ":param path: string, or list of strings, for input path(s),\n",
    "             or RDD of Strings storing CSV rows.\n",
    ":param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
    "               or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
    ":param sep: sets a single character as a separator for each field and value.\n",
    "            If None is set, it uses the default value, ``,``.\n",
    ":param encoding: decodes the CSV files by the given encoding type. If None is set,\n",
    "                 it uses the default value, ``UTF-8``.\n",
    ":param quote: sets a single character used for escaping quoted values where the\n",
    "              separator can be part of the value. If None is set, it uses the default\n",
    "              value, ``\"``. If you would like to turn off quotations, you need to set an\n",
    "              empty string.\n",
    ":param escape: sets a single character used for escaping quotes inside an already\n",
    "               quoted value. If None is set, it uses the default value, ``\\``.\n",
    ":param comment: sets a single character used for skipping lines beginning with this\n",
    "                character. By default (None), it is disabled.\n",
    ":param header: uses the first line as names of columns. If None is set, it uses the\n",
    "               default value, ``false``.\n",
    ":param inferSchema: infers the input schema automatically from data. It requires one extra\n",
    "               pass over the data. If None is set, it uses the default value, ``false``.\n",
    ":param enforceSchema: If it is set to ``true``, the specified or inferred schema will be\n",
    "                      forcibly applied to datasource files, and headers in CSV files will be\n",
    "                      ignored. If the option is set to ``false``, the schema will be\n",
    "                      validated against all headers in CSV files or the first header in RDD\n",
    "                      if the ``header`` option is set to ``true``. Field names in the schema\n",
    "                      and column names in CSV headers are checked by their positions\n",
    "                      taking into account ``spark.sql.caseSensitive``. If None is set,\n",
    "                      ``true`` is used by default. Though the default value is ``true``,\n",
    "                      it is recommended to disable the ``enforceSchema`` option\n",
    "                      to avoid incorrect results.\n",
    ":param ignoreLeadingWhiteSpace: A flag indicating whether or not leading whitespaces from\n",
    "                                values being read should be skipped. If None is set, it\n",
    "                                uses the default value, ``false``.\n",
    ":param ignoreTrailingWhiteSpace: A flag indicating whether or not trailing whitespaces from\n",
    "                                 values being read should be skipped. If None is set, it\n",
    "                                 uses the default value, ``false``.\n",
    ":param nullValue: sets the string representation of a null value. If None is set, it uses\n",
    "                  the default value, empty string. Since 2.0.1, this ``nullValue`` param\n",
    "                  applies to all supported types including the string type.\n",
    ":param nanValue: sets the string representation of a non-number value. If None is set, it\n",
    "                 uses the default value, ``NaN``.\n",
    ":param positiveInf: sets the string representation of a positive infinity value. If None\n",
    "                    is set, it uses the default value, ``Inf``.\n",
    ":param negativeInf: sets the string representation of a negative infinity value. If None\n",
    "                    is set, it uses the default value, ``Inf``.\n",
    ":param dateFormat: sets the string that indicates a date format. Custom date formats\n",
    "                   follow the formats at ``java.text.SimpleDateFormat``. This\n",
    "                   applies to date type. If None is set, it uses the\n",
    "                   default value, ``yyyy-MM-dd``.\n",
    ":param timestampFormat: sets the string that indicates a timestamp format. Custom date\n",
    "                        formats follow the formats at ``java.text.SimpleDateFormat``.\n",
    "                        This applies to timestamp type. If None is set, it uses the\n",
    "                        default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\n",
    ":param maxColumns: defines a hard limit of how many columns a record can have. If None is\n",
    "                   set, it uses the default value, ``20480``.\n",
    ":param maxCharsPerColumn: defines the maximum number of characters allowed for any given\n",
    "                          value being read. If None is set, it uses the default value,\n",
    "                          ``-1`` meaning unlimited length.\n",
    ":param maxMalformedLogPerPartition: this parameter is no longer used since Spark 2.2.0.\n",
    "                                    If specified, it is ignored.\n",
    ":param mode: allows a mode for dealing with corrupt records during parsing. If None is\n",
    "             set, it uses the default value, ``PERMISSIVE``.\n",
    "\n",
    "        * ``PERMISSIVE`` : when it meets a corrupted record, puts the malformed string \\\n",
    "          into a field configured by ``columnNameOfCorruptRecord``, and sets other \\\n",
    "          fields to ``null``. To keep corrupt records, an user can set a string type \\\n",
    "          field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a \\\n",
    "          schema does not have the field, it drops corrupt records during parsing. \\\n",
    "          A record with less/more tokens than schema is not a corrupted record to CSV. \\\n",
    "          When it meets a record having fewer tokens than the length of the schema, \\\n",
    "          sets ``null`` to extra fields. When the record has more tokens than the \\\n",
    "          length of the schema, it drops extra tokens.\n",
    "        * ``DROPMALFORMED`` : ignores the whole corrupted records.\n",
    "        * ``FAILFAST`` : throws an exception when it meets corrupted records.\n",
    "\n",
    ":param columnNameOfCorruptRecord: allows renaming the new field having malformed string\n",
    "                                  created by ``PERMISSIVE`` mode. This overrides\n",
    "                                  ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n",
    "                                  it uses the value specified in\n",
    "                                  ``spark.sql.columnNameOfCorruptRecord``.\n",
    ":param multiLine: parse records, which may span multiple lines. If None is\n",
    "                  set, it uses the default value, ``false``.\n",
    ":param charToEscapeQuoteEscaping: sets a single character used for escaping the escape for\n",
    "                                  the quote character. If None is set, the default value is\n",
    "                                  escape character when escape and quote characters are\n",
    "                                  different, ``\\0`` otherwise.\n",
    ":param samplingRatio: defines fraction of rows used for schema inferring.\n",
    "                      If None is set, it uses the default value, ``1.0``.\n",
    ":param emptyValue: sets the string representation of an empty value. If None is set, it uses\n",
    "                   the default value, empty string.\n",
    "\n",
    ">>> df = spark.read.csv('python/test_support/sql/ages.csv')\n",
    ">>> df.dtypes\n",
    "[('_c0', 'string'), ('_c1', 'string')]\n",
    ">>> rdd = sc.textFile('python/test_support/sql/ages.csv')\n",
    ">>> df2 = spark.read.csv(rdd)\n",
    ">>> df2.dtypes\n",
    "[('_c0', 'string'), ('_c1', 'string')]\n",
    "\n",
    ".. versionadded:: 2.0\n",
    "File:      /opt/spark/python/pyspark/sql/readwriter.py\n",
    "Type:      method\n",
    "\n",
    "\"\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
