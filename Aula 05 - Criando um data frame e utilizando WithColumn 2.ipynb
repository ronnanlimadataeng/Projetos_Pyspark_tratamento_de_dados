{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando um data frame e utilizando WithColumn 2 - Aula 5\n",
    "1. Criar um dataframe para ler o arquivo no HDFS /user/<nome/data/juros_selic/juros_selic\n",
    "2. Agrupar todas as datas pelo ano em ordem decrescente e salvar a quantidade de meses ocorridos, o valor médio, mínimo e máximo do campo valor com a seguinte estrutura:\n",
    "\n",
    "ANUAL | MESES | VALOR MÉDIO | VALOR MÍNIMO | VALOR MÁXIMO\n",
    "2019  | 2     | 00.00       | 00.00        | 00.00\n",
    "2018  | 12    | 00.00       | 00.00        | 00.00\n",
    "2017  | 12    | 00.00       | 00.00        | 00.00\n",
    "...   | ...   | 00.00       | 00.00        | 00.00\n",
    "1986  | 2     | 00.00       | 00.00        | 00.00\n",
    "\n",
    "3. Salvar no hdfs:///user/<nome>/relatorioAnual com compressão zlib e formato orc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   3 root supergroup       7954 2022-04-06 03:19 /user/ronnan/data/juros_selic/juros_selic\r\n"
     ]
    }
   ],
   "source": [
    "#verificando se o caminho do arquivo está correto\n",
    "!hdfs dfs -ls /user/ronnan/data/juros_selic/juros_selic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"data\";\"valor\"\r\n",
      "\"01/06/1986\";\"1,27\"\r\n",
      "\"01/07/1986\";\"1,95\"\r\n",
      "\"01/08/1986\";\"2,57\"\r\n",
      "\"01/09/1986\";\"2,94\"\r\n",
      "\"01/10/1986\";\"1,96\"\r\n",
      "\"01/11/1986\";\"2,37\"\r\n",
      "\"01/12/1986\";\"5,47\"\r\n",
      "\"01/01/1987\";\"11,00\"\r\n",
      "\"01/02/1987\";\"19,61\"\r\n",
      "\"01/03/1987\";\"11,95\"\r\n",
      "\"01/04/1987\";\"15,30\"\r\n",
      "\"01/05/1987\";\"24,63\"\r\n",
      "\"01/06/1987\";\"18,02\"\r\n",
      "\"01/07/1987\";\"8,91\"\r\n",
      "\"01/08/1987\";\"8,09\"\r\n",
      "\"01/09/1987\";\"7,99\"\r\n",
      "\"01/10/1987\";\"9,45\"\r\n",
      "\"01/11/1987\";\"12,92\"\r\n",
      "\"01/12/1987\";\"14,38\"\r\n",
      "\"01/01/1988\";\"16,78\"\r\n",
      "\"01/02/1988\";\"18,35\"\r\n",
      "\"01/03/1988\";\"16,59\"\r\n",
      "\"01/04/1988\";\"20,25\"\r\n",
      "\"01/05/1988\";\"18,65\"\r\n",
      "\"01/06/1988\";\"20,17\"\r\n",
      "\"01/07/1988\";\"24,69\"\r\n",
      "\"01/08/1988\";\"22,63\"\r\n",
      "\"01/09/1988\";\"26,25\"\r\n",
      "\"01/10/1988\";\"29,79\"\r\n",
      "\"01/11/1988\";\"28,41\"\r\n",
      "\"01/12/1988\";\"30,24\"\r\n",
      "\"01/01/1989\";\"22,97\"\r\n",
      "\"01/02/1989\";\"18,95\"\r\n",
      "\"01/03/1989\";\"20,41\"\r\n",
      "\"01/04/1989\";\"11,52\"\r\n",
      "\"01/05/1989\";\"11,43\"\r\n",
      "\"01/06/1989\";\"27,29\"\r\n",
      "\"01/07/1989\";\"33,15\"\r\n",
      "\"01/08/1989\";\"35,49\"\r\n",
      "\"01/09/1989\";\"38,58\"\r\n",
      "\"01/10/1989\";\"47,70\"\r\n",
      "\"01/11/1989\";\"48,41\"\r\n",
      "\"01/12/1989\";\"64,21\"\r\n",
      "\"01/01/1990\";\"67,60\"\r\n",
      "\"01/02/1990\";\"82,04\"\r\n",
      "\"01/03/1990\";\"36,76\"\r\n",
      "\"01/04/1990\";\"4,23\"\r\n",
      "\"01/05/1990\";\"5,69\"\r\n",
      "\"01/06/1990\";\"8,73\"\r\n",
      "\"01/07/1990\";\"13,79\"\r\n",
      "\"01/08/1990\";\"11,53\"\r\n",
      "\"01/09/1990\";\"15,21\"\r\n",
      "\"01/10/1990\";\"16,49\"\r\n",
      "\"01/11/1990\";\"19,83\"\r\n",
      "\"01/12/1990\";\"22,86\"\r\n",
      "\"01/01/1991\";\"21,02\"\r\n",
      "\"01/02/1991\";\"6,85\"\r\n",
      "\"01/03/1991\";\"8,99\"\r\n",
      "\"01/04/1991\";\"9,67\"\r\n",
      "\"01/05/1991\";\"9,56\"\r\n",
      "\"01/06/1991\";\"10,32\"\r\n",
      "\"01/07/1991\";\"12,39\"\r\n",
      "\"01/08/1991\";\"15,75\"\r\n",
      "\"01/09/1991\";\"19,78\"\r\n",
      "\"01/10/1991\";\"25,95\"\r\n",
      "\"01/11/1991\";\"32,43\"\r\n",
      "\"01/12/1991\";\"31,17\"\r\n",
      "\"01/01/1992\";\"29,06\"\r\n",
      "\"01/02/1992\";\"28,76\"\r\n",
      "\"01/03/1992\";\"26,86\"\r\n",
      "\"01/04/1992\";\"23,92\"\r\n",
      "\"01/05/1992\";\"23,00\"\r\n",
      "\"01/06/1992\";\"24,28\"\r\n",
      "\"01/07/1992\";\"26,21\"\r\n",
      "\"01/08/1992\";\"25,65\"\r\n",
      "\"01/09/1992\";\"27,66\"\r\n",
      "\"01/10/1992\";\"28,18\"\r\n",
      "\"01/11/1992\";\"26,40\"\r\n",
      "\"01/12/1992\";\"25,92\"\r\n",
      "\"01/01/1993\";\"28,52\"\r\n",
      "\"01/02/1993\";\"28,90\"\r\n",
      "\"01/03/1993\";\"28,36\"\r\n",
      "\"01/04/1993\";\"30,53\"\r\n",
      "\"01/05/1993\";\"30,90\"\r\n",
      "\"01/06/1993\";\"31,91\"\r\n",
      "\"01/07/1993\";\"32,73\"\r\n",
      "\"01/08/1993\";\"34,64\"\r\n",
      "\"01/09/1993\";\"37,23\"\r\n",
      "\"01/10/1993\";\"38,40\"\r\n",
      "\"01/11/1993\";\"38,38\"\r\n",
      "\"01/12/1993\";\"40,38\"\r\n",
      "\"01/01/1994\";\"42,76\"\r\n",
      "\"01/02/1994\";\"41,99\"\r\n",
      "\"01/03/1994\";\"46,42\"\r\n",
      "\"01/04/1994\";\"46,49\"\r\n",
      "\"01/05/1994\";\"47,95\"\r\n",
      "\"01/06/1994\";\"50,62\"\r\n",
      "\"01/07/1994\";\"6,87\"\r\n",
      "\"01/08/1994\";\"4,17\"\r\n",
      "\"01/09/1994\";\"3,83\"\r\n",
      "\"01/10/1994\";\"3,62\"\r\n",
      "\"01/11/1994\";\"4,07\"\r\n",
      "\"01/12/1994\";\"3,80\"\r\n",
      "\"01/01/1995\";\"3,37\"\r\n",
      "\"01/02/1995\";\"3,25\"\r\n",
      "\"01/03/1995\";\"4,26\"\r\n",
      "\"01/04/1995\";\"4,26\"\r\n",
      "\"01/05/1995\";\"4,25\"\r\n",
      "\"01/06/1995\";\"4,04\"\r\n",
      "\"01/07/1995\";\"4,02\"\r\n",
      "\"01/08/1995\";\"3,84\"\r\n",
      "\"01/09/1995\";\"3,32\"\r\n",
      "\"01/10/1995\";\"3,09\"\r\n",
      "\"01/11/1995\";\"2,88\"\r\n",
      "\"01/12/1995\";\"2,78\"\r\n",
      "\"01/01/1996\";\"2,58\"\r\n",
      "\"01/02/1996\";\"2,35\"\r\n",
      "\"01/03/1996\";\"2,22\"\r\n",
      "\"01/04/1996\";\"2,07\"\r\n",
      "\"01/05/1996\";\"2,01\"\r\n",
      "\"01/06/1996\";\"1,98\"\r\n",
      "\"01/07/1996\";\"1,93\"\r\n",
      "\"01/08/1996\";\"1,97\"\r\n",
      "\"01/09/1996\";\"1,90\"\r\n",
      "\"01/10/1996\";\"1,86\"\r\n",
      "\"01/11/1996\";\"1,80\"\r\n",
      "\"01/12/1996\";\"1,80\"\r\n",
      "\"01/01/1997\";\"1,73\"\r\n",
      "\"01/02/1997\";\"1,67\"\r\n",
      "\"01/03/1997\";\"1,64\"\r\n",
      "\"01/04/1997\";\"1,66\"\r\n",
      "\"01/05/1997\";\"1,58\"\r\n",
      "\"01/06/1997\";\"1,61\"\r\n",
      "\"01/07/1997\";\"1,60\"\r\n",
      "\"01/08/1997\";\"1,59\"\r\n",
      "\"01/09/1997\";\"1,59\"\r\n",
      "\"01/10/1997\";\"1,67\"\r\n",
      "\"01/11/1997\";\"3,04\"\r\n",
      "\"01/12/1997\";\"2,97\"\r\n",
      "\"01/01/1998\";\"2,67\"\r\n",
      "\"01/02/1998\";\"2,13\"\r\n",
      "\"01/03/1998\";\"2,20\"\r\n",
      "\"01/04/1998\";\"1,71\"\r\n",
      "\"01/05/1998\";\"1,63\"\r\n",
      "\"01/06/1998\";\"1,60\"\r\n",
      "\"01/07/1998\";\"1,70\"\r\n",
      "\"01/08/1998\";\"1,48\"\r\n",
      "\"01/09/1998\";\"2,49\"\r\n",
      "\"01/10/1998\";\"2,94\"\r\n",
      "\"01/11/1998\";\"2,63\"\r\n",
      "\"01/12/1998\";\"2,40\"\r\n",
      "\"01/01/1999\";\"2,18\"\r\n",
      "\"01/02/1999\";\"2,38\"\r\n",
      "\"01/03/1999\";\"3,33\"\r\n",
      "\"01/04/1999\";\"2,35\"\r\n",
      "\"01/05/1999\";\"2,02\"\r\n",
      "\"01/06/1999\";\"1,67\"\r\n",
      "\"01/07/1999\";\"1,66\"\r\n",
      "\"01/08/1999\";\"1,57\"\r\n",
      "\"01/09/1999\";\"1,49\"\r\n",
      "\"01/10/1999\";\"1,38\"\r\n",
      "\"01/11/1999\";\"1,39\"\r\n",
      "\"01/12/1999\";\"1,60\"\r\n",
      "\"01/01/2000\";\"1,46\"\r\n",
      "\"01/02/2000\";\"1,45\"\r\n",
      "\"01/03/2000\";\"1,45\"\r\n",
      "\"01/04/2000\";\"1,30\"\r\n",
      "\"01/05/2000\";\"1,49\"\r\n",
      "\"01/06/2000\";\"1,39\"\r\n",
      "\"01/07/2000\";\"1,31\"\r\n",
      "\"01/08/2000\";\"1,41\"\r\n",
      "\"01/09/2000\";\"1,22\"\r\n",
      "\"01/10/2000\";\"1,29\"\r\n",
      "\"01/11/2000\";\"1,22\"\r\n",
      "\"01/12/2000\";\"1,20\"\r\n",
      "\"01/01/2001\";\"1,27\"\r\n",
      "\"01/02/2001\";\"1,02\"\r\n",
      "\"01/03/2001\";\"1,26\"\r\n",
      "\"01/04/2001\";\"1,19\"\r\n",
      "\"01/05/2001\";\"1,34\"\r\n",
      "\"01/06/2001\";\"1,27\"\r\n",
      "\"01/07/2001\";\"1,50\"\r\n",
      "\"01/08/2001\";\"1,60\"\r\n",
      "\"01/09/2001\";\"1,32\"\r\n",
      "\"01/10/2001\";\"1,53\"\r\n",
      "\"01/11/2001\";\"1,39\"\r\n",
      "\"01/12/2001\";\"1,39\"\r\n",
      "\"01/01/2002\";\"1,53\"\r\n",
      "\"01/02/2002\";\"1,25\"\r\n",
      "\"01/03/2002\";\"1,37\"\r\n",
      "\"01/04/2002\";\"1,48\"\r\n",
      "\"01/05/2002\";\"1,41\"\r\n",
      "\"01/06/2002\";\"1,33\"\r\n",
      "\"01/07/2002\";\"1,54\"\r\n",
      "\"01/08/2002\";\"1,44\"\r\n",
      "\"01/09/2002\";\"1,38\"\r\n",
      "\"01/10/2002\";\"1,65\"\r\n",
      "\"01/11/2002\";\"1,54\"\r\n",
      "\"01/12/2002\";\"1,74\"\r\n",
      "\"01/01/2003\";\"1,97\"\r\n",
      "\"01/02/2003\";\"1,83\"\r\n",
      "\"01/03/2003\";\"1,78\"\r\n",
      "\"01/04/2003\";\"1,87\"\r\n",
      "\"01/05/2003\";\"1,97\"\r\n",
      "\"01/06/2003\";\"1,86\"\r\n",
      "\"01/07/2003\";\"2,08\"\r\n",
      "\"01/08/2003\";\"1,77\"\r\n",
      "\"01/09/2003\";\"1,68\"\r\n",
      "\"01/10/2003\";\"1,64\"\r\n",
      "\"01/11/2003\";\"1,34\"\r\n",
      "\"01/12/2003\";\"1,37\"\r\n",
      "\"01/01/2004\";\"1,27\"\r\n",
      "\"01/02/2004\";\"1,08\"\r\n",
      "\"01/03/2004\";\"1,38\"\r\n",
      "\"01/04/2004\";\"1,18\"\r\n",
      "\"01/05/2004\";\"1,23\"\r\n",
      "\"01/06/2004\";\"1,23\"\r\n",
      "\"01/07/2004\";\"1,29\"\r\n",
      "\"01/08/2004\";\"1,29\"\r\n",
      "\"01/09/2004\";\"1,25\"\r\n",
      "\"01/10/2004\";\"1,21\"\r\n",
      "\"01/11/2004\";\"1,25\"\r\n",
      "\"01/12/2004\";\"1,48\"\r\n",
      "\"01/01/2005\";\"1,38\"\r\n",
      "\"01/02/2005\";\"1,22\"\r\n",
      "\"01/03/2005\";\"1,53\"\r\n",
      "\"01/04/2005\";\"1,41\"\r\n",
      "\"01/05/2005\";\"1,50\"\r\n",
      "\"01/06/2005\";\"1,59\"\r\n",
      "\"01/07/2005\";\"1,51\"\r\n",
      "\"01/08/2005\";\"1,66\"\r\n",
      "\"01/09/2005\";\"1,50\"\r\n",
      "\"01/10/2005\";\"1,41\"\r\n",
      "\"01/11/2005\";\"1,38\"\r\n",
      "\"01/12/2005\";\"1,47\"\r\n",
      "\"01/01/2006\";\"1,43\"\r\n",
      "\"01/02/2006\";\"1,15\"\r\n",
      "\"01/03/2006\";\"1,42\"\r\n",
      "\"01/04/2006\";\"1,08\"\r\n",
      "\"01/05/2006\";\"1,28\"\r\n",
      "\"01/06/2006\";\"1,18\"\r\n",
      "\"01/07/2006\";\"1,17\"\r\n",
      "\"01/08/2006\";\"1,26\"\r\n",
      "\"01/09/2006\";\"1,06\"\r\n",
      "\"01/10/2006\";\"1,09\"\r\n",
      "\"01/11/2006\";\"1,02\"\r\n",
      "\"01/12/2006\";\"0,99\"\r\n",
      "\"01/01/2007\";\"1,08\"\r\n",
      "\"01/02/2007\";\"0,87\"\r\n",
      "\"01/03/2007\";\"1,05\"\r\n",
      "\"01/04/2007\";\"0,94\"\r\n",
      "\"01/05/2007\";\"1,03\"\r\n",
      "\"01/06/2007\";\"0,91\"\r\n",
      "\"01/07/2007\";\"0,97\"\r\n",
      "\"01/08/2007\";\"0,99\"\r\n",
      "\"01/09/2007\";\"0,80\"\r\n",
      "\"01/10/2007\";\"0,93\"\r\n",
      "\"01/11/2007\";\"0,84\"\r\n",
      "\"01/12/2007\";\"0,84\"\r\n",
      "\"01/01/2008\";\"0,93\"\r\n",
      "\"01/02/2008\";\"0,80\"\r\n",
      "\"01/03/2008\";\"0,84\"\r\n",
      "\"01/04/2008\";\"0,90\"\r\n",
      "\"01/05/2008\";\"0,88\"\r\n",
      "\"01/06/2008\";\"0,96\"\r\n",
      "\"01/07/2008\";\"1,07\"\r\n",
      "\"01/08/2008\";\"1,02\"\r\n",
      "\"01/09/2008\";\"1,10\"\r\n",
      "\"01/10/2008\";\"1,18\"\r\n",
      "\"01/11/2008\";\"1,02\"\r\n",
      "\"01/12/2008\";\"1,12\"\r\n",
      "\"01/01/2009\";\"1,05\"\r\n",
      "\"01/02/2009\";\"0,86\"\r\n",
      "\"01/03/2009\";\"0,97\"\r\n",
      "\"01/04/2009\";\"0,84\"\r\n",
      "\"01/05/2009\";\"0,77\"\r\n",
      "\"01/06/2009\";\"0,76\"\r\n",
      "\"01/07/2009\";\"0,79\"\r\n",
      "\"01/08/2009\";\"0,69\"\r\n",
      "\"01/09/2009\";\"0,69\"\r\n",
      "\"01/10/2009\";\"0,69\"\r\n",
      "\"01/11/2009\";\"0,66\"\r\n",
      "\"01/12/2009\";\"0,73\"\r\n",
      "\"01/01/2010\";\"0,66\"\r\n",
      "\"01/02/2010\";\"0,59\"\r\n",
      "\"01/03/2010\";\"0,76\"\r\n",
      "\"01/04/2010\";\"0,67\"\r\n",
      "\"01/05/2010\";\"0,75\"\r\n",
      "\"01/06/2010\";\"0,79\"\r\n",
      "\"01/07/2010\";\"0,86\"\r\n",
      "\"01/08/2010\";\"0,89\"\r\n",
      "\"01/09/2010\";\"0,85\"\r\n",
      "\"01/10/2010\";\"0,81\"\r\n",
      "\"01/11/2010\";\"0,81\"\r\n",
      "\"01/12/2010\";\"0,93\"\r\n",
      "\"01/01/2011\";\"0,86\"\r\n",
      "\"01/02/2011\";\"0,84\"\r\n",
      "\"01/03/2011\";\"0,92\"\r\n",
      "\"01/04/2011\";\"0,84\"\r\n",
      "\"01/05/2011\";\"0,99\"\r\n",
      "\"01/06/2011\";\"0,96\"\r\n",
      "\"01/07/2011\";\"0,97\"\r\n",
      "\"01/08/2011\";\"1,07\"\r\n",
      "\"01/09/2011\";\"0,94\"\r\n",
      "\"01/10/2011\";\"0,88\"\r\n",
      "\"01/11/2011\";\"0,86\"\r\n",
      "\"01/12/2011\";\"0,91\"\r\n",
      "\"01/01/2012\";\"0,89\"\r\n",
      "\"01/02/2012\";\"0,75\"\r\n",
      "\"01/03/2012\";\"0,82\"\r\n",
      "\"01/04/2012\";\"0,71\"\r\n",
      "\"01/05/2012\";\"0,74\"\r\n",
      "\"01/06/2012\";\"0,64\"\r\n",
      "\"01/07/2012\";\"0,68\"\r\n",
      "\"01/08/2012\";\"0,69\"\r\n",
      "\"01/09/2012\";\"0,54\"\r\n",
      "\"01/10/2012\";\"0,61\"\r\n",
      "\"01/11/2012\";\"0,55\"\r\n",
      "\"01/12/2012\";\"0,55\"\r\n",
      "\"01/01/2013\";\"0,60\"\r\n",
      "\"01/02/2013\";\"0,49\"\r\n",
      "\"01/03/2013\";\"0,55\"\r\n",
      "\"01/04/2013\";\"0,61\"\r\n",
      "\"01/05/2013\";\"0,60\"\r\n",
      "\"01/06/2013\";\"0,61\"\r\n",
      "\"01/07/2013\";\"0,72\"\r\n",
      "\"01/08/2013\";\"0,71\"\r\n",
      "\"01/09/2013\";\"0,71\"\r\n",
      "\"01/10/2013\";\"0,81\"\r\n",
      "\"01/11/2013\";\"0,72\"\r\n",
      "\"01/12/2013\";\"0,79\"\r\n",
      "\"01/01/2014\";\"0,85\"\r\n",
      "\"01/02/2014\";\"0,79\"\r\n",
      "\"01/03/2014\";\"0,77\"\r\n",
      "\"01/04/2014\";\"0,82\"\r\n",
      "\"01/05/2014\";\"0,87\"\r\n",
      "\"01/06/2014\";\"0,82\"\r\n",
      "\"01/07/2014\";\"0,95\"\r\n",
      "\"01/08/2014\";\"0,87\"\r\n",
      "\"01/09/2014\";\"0,91\"\r\n",
      "\"01/10/2014\";\"0,95\"\r\n",
      "\"01/11/2014\";\"0,84\"\r\n",
      "\"01/12/2014\";\"0,96\"\r\n",
      "\"01/01/2015\";\"0,94\"\r\n",
      "\"01/02/2015\";\"0,82\"\r\n",
      "\"01/03/2015\";\"1,04\"\r\n",
      "\"01/04/2015\";\"0,95\"\r\n",
      "\"01/05/2015\";\"0,99\"\r\n",
      "\"01/06/2015\";\"1,07\"\r\n",
      "\"01/07/2015\";\"1,18\"\r\n",
      "\"01/08/2015\";\"1,11\"\r\n",
      "\"01/09/2015\";\"1,11\"\r\n",
      "\"01/10/2015\";\"1,11\"\r\n",
      "\"01/11/2015\";\"1,06\"\r\n",
      "\"01/12/2015\";\"1,16\"\r\n",
      "\"01/01/2016\";\"1,06\"\r\n",
      "\"01/02/2016\";\"1,00\"\r\n",
      "\"01/03/2016\";\"1,16\"\r\n",
      "\"01/04/2016\";\"1,06\"\r\n",
      "\"01/05/2016\";\"1,11\"\r\n",
      "\"01/06/2016\";\"1,16\"\r\n",
      "\"01/07/2016\";\"1,11\"\r\n",
      "\"01/08/2016\";\"1,22\"\r\n",
      "\"01/09/2016\";\"1,11\"\r\n",
      "\"01/10/2016\";\"1,05\"\r\n",
      "\"01/11/2016\";\"1,04\"\r\n",
      "\"01/12/2016\";\"1,12\"\r\n",
      "\"01/01/2017\";\"1,09\"\r\n",
      "\"01/02/2017\";\"0,87\"\r\n",
      "\"01/03/2017\";\"1,05\"\r\n",
      "\"01/04/2017\";\"0,79\"\r\n",
      "\"01/05/2017\";\"0,93\"\r\n",
      "\"01/06/2017\";\"0,81\"\r\n",
      "\"01/07/2017\";\"0,80\"\r\n",
      "\"01/08/2017\";\"0,80\"\r\n",
      "\"01/09/2017\";\"0,64\"\r\n",
      "\"01/10/2017\";\"0,64\"\r\n",
      "\"01/11/2017\";\"0,57\"\r\n",
      "\"01/12/2017\";\"0,54\"\r\n",
      "\"01/01/2018\";\"0,58\"\r\n",
      "\"01/02/2018\";\"0,47\"\r\n",
      "\"01/03/2018\";\"0,53\"\r\n",
      "\"01/04/2018\";\"0,52\"\r\n",
      "\"01/05/2018\";\"0,52\"\r\n",
      "\"01/06/2018\";\"0,52\"\r\n",
      "\"01/07/2018\";\"0,54\"\r\n",
      "\"01/08/2018\";\"0,57\"\r\n",
      "\"01/09/2018\";\"0,47\"\r\n",
      "\"01/10/2018\";\"0,54\"\r\n",
      "\"01/11/2018\";\"0,49\"\r\n",
      "\"01/12/2018\";\"0,49\"\r\n",
      "\"01/01/2019\";\"0,54\"\r\n",
      "\"01/02/2019\";\"0,37\"\r\n"
     ]
    }
   ],
   "source": [
    "#lendo o arquivo juros_selic\n",
    "!hdfs dfs -cat /user/ronnan/data/juros_selic/juros_selic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Criar um dataframe para ler o arquivo no HDFS /user/<nome/data/juros_selic/juros_selic\n",
    "juros = spark.read.csv(\"/user/ronnan/data/juros_selic/juros_selic\",sep= \";\",header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data: string (nullable = true)\n",
      " |-- valor: string (nullable = true)\n",
      "\n",
      "None\n",
      "+----------+-----+\n",
      "|      data|valor|\n",
      "+----------+-----+\n",
      "|01/06/1986| 1,27|\n",
      "|01/07/1986| 1,95|\n",
      "|01/08/1986| 2,57|\n",
      "|01/09/1986| 2,94|\n",
      "|01/10/1986| 1,96|\n",
      "+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#visualiar o schema e visualizar os 5 primeiros\n",
    "print(juros.printSchema())\n",
    "juros.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.tests import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#2. Agrupar todas as datas pelo ano em ordem decrescente e salvar a quantidade de meses ocorridos, o valor médio, mínimo e máximo do campo valor com a seguinte estrutura:\n",
    "juros_ano = juros.withColumn(\"ano\", split(col(\"data\"),\"/\").getItem(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data: string (nullable = true)\n",
      " |-- valor: float (nullable = true)\n",
      " |-- ano: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#substituindo a virgula por ponto do campo valor / use  cast para mudar o formato de string para Float\n",
    "# lembre se de importar o types\n",
    "juros_valor = juros_ano.withColumn(\"valor\",regexp_replace(col(\"valor\"),\"\\,\",\"\\.\").cast(FloatType()))\n",
    "juros_valor.printSchema() #note que o campo valor foi anterado de string para Float(valor com virgula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# utilizando o agg para somar com count os meses, depois alias para renomear coluna\n",
    "# avg para inserir a media e alias para renomear coluna e format_number para reduzir 2 casas\n",
    "# min para acrescentar o valor minimo\n",
    "# max para acrescentar o valor maximo\n",
    "#sort para ordenar pelo ano desc(descrescente)\n",
    "\n",
    "juros_relatorio = juros_valor.groupBy(\"ano\").agg(count(\"ano\").alias(\"Meses\"),\n",
    "format_number(avg(\"valor\"),2).alias(\"Valor Médio\"), min(\"valor\").alias(\"valor Mínimo\"),\n",
    "max(\"valor\").alias(\"Valor Máximo\")).sort(desc(\"ano\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Salvar no hdfs:///user/<nome>/relatorioAnual com compressão zlib e formato orc\n",
    "juros_relatorio.write.orc(\"/user/ronnan/relatorio_anual\", compression= \"zlib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35 items\n",
      "-rw-r--r--   2 root supergroup          0 2022-04-10 21:21 /user/ronnan/relatorio_anual/_SUCCESS\n",
      "-rw-r--r--   2 root supergroup        585 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00000-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        585 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00001-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        588 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00002-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        575 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00003-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        586 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00004-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        585 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00005-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        582 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00006-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        585 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00007-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        588 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00008-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        588 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00009-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        589 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00010-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        586 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00011-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        589 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00012-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        588 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00013-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        585 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00014-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        585 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00015-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        587 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00016-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        575 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00017-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        589 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00018-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        583 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00019-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        583 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00020-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        585 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00021-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        583 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00022-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        588 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00023-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        584 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00024-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        608 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00025-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        605 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00026-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        599 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00027-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        608 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00028-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        608 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00029-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        605 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00030-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        608 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00031-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        609 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00032-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n",
      "-rw-r--r--   2 root supergroup        585 2022-04-10 21:21 /user/ronnan/relatorio_anual/part-00033-bdaefbb4-c1de-4612-ae17-8132b62564be-c000.zlib.orc\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/ronnan/relatorio_anual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35 items\n",
      "-rw-r--r--   2 root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/_SUCCESS\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=1986\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=1987\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=1988\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=1989\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=1990\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=1991\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=1992\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=1993\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=1994\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=1995\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=1996\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=1997\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=1998\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=1999\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=2000\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=2001\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=2002\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=2003\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=2004\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=2005\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=2006\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=2007\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=2008\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=2009\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=2010\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=2011\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=2012\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=2013\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=2014\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=2015\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=2016\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=2017\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=2018\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=2019\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "!hdfs dfs -ls /user/ronnan/relatorio_anual_particionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-08 00:39 /user/hive/warehouse/juros\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "!hdfs dfs -ls /user/hive/warehouse/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#3 alternativa(\"clayton\"). Salvar no hdfs:///user/<nome>/relatorioAnual com compressão zlib e formato orc\n",
    "# para criar partições com base no campo ano partitionBy\n",
    "juros_relatorio.write.orc(\"/user/ronnan/relatorio_anual_particionado\", partitionBy=\"ano\", compression= \"zlib\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x   - root supergroup          0 2022-04-10 21:44 /user/ronnan/relatorio_anual_particionado/ano=2018\r\n"
     ]
    }
   ],
   "source": [
    "# verificar se o arquivo no ano de 2018 pode ser localizar\n",
    "!hdfs dfs -ls /user/ronnan/relatorio_anual_particionado | grep 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o957.saveAsTable.\n: java.lang.NoClassDefFoundError: parquet/hadoop/ParquetOutputFormat\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:238)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$.org$apache$spark$sql$hive$client$HiveClientImpl$$toOutputFormat(HiveClientImpl.scala:913)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$toHiveTable$6.apply(HiveClientImpl.scala:947)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$toHiveTable$6.apply(HiveClientImpl.scala:947)\n\tat scala.Option.map(Option.scala:146)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$.toHiveTable(HiveClientImpl.scala:947)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply$mcV$sp(HiveClientImpl.scala:482)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply(HiveClientImpl.scala:480)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply(HiveClientImpl.scala:480)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:480)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:499)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.org$apache$spark$sql$hive$HiveExternalCatalog$$createDataSourceTable(HiveExternalCatalog.scala:387)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply$mcV$sp(HiveExternalCatalog.scala:263)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:236)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:236)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:236)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:324)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:185)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:465)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:444)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:400)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: parquet.hadoop.ParquetOutputFormat\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.doLoadClass(IsolatedClientLoader.scala:226)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.loadClass(IsolatedClientLoader.scala:215)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\t... 57 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-63f5b664b3bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# salvando a tabela relatorio dentro do hive conforme as partições\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mjuros_hive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjuros_relatorio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Valor médio\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"valor_medio\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Valor Máximo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"valor_maximo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Valor Mínimo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"valor_minimo\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mjuros_hive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"relatorioanualparticionado\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitionBy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ano\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"OverWrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o957.saveAsTable.\n: java.lang.NoClassDefFoundError: parquet/hadoop/ParquetOutputFormat\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:348)\n\tat org.apache.spark.util.Utils$.classForName(Utils.scala:238)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$.org$apache$spark$sql$hive$client$HiveClientImpl$$toOutputFormat(HiveClientImpl.scala:913)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$toHiveTable$6.apply(HiveClientImpl.scala:947)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$toHiveTable$6.apply(HiveClientImpl.scala:947)\n\tat scala.Option.map(Option.scala:146)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$.toHiveTable(HiveClientImpl.scala:947)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply$mcV$sp(HiveClientImpl.scala:482)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply(HiveClientImpl.scala:480)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply(HiveClientImpl.scala:480)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:480)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:499)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.org$apache$spark$sql$hive$HiveExternalCatalog$$createDataSourceTable(HiveExternalCatalog.scala:387)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply$mcV$sp(HiveExternalCatalog.scala:263)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:236)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:236)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:236)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:324)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:185)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:465)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:444)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:400)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: parquet.hadoop.ParquetOutputFormat\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.doLoadClass(IsolatedClientLoader.scala:226)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.loadClass(IsolatedClientLoader.scala:215)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\t... 57 more\n"
     ]
    }
   ],
   "source": [
    "# salvando a tabela relatorio dentro do hive conforme as partições\n",
    "juros_hive = juros_relatorio.withColumnRenamed(\"Valor médio\", \"valor_medio\").withColumnRenamed(\"Valor Máximo\",\"valor_maximo\").withColumnRenamed(\"Valor Mínimo\",\"valor_minimo\")\n",
    "juros_hive.write.saveAsTable(\"relatorioanualparticionado\", partitionBy=\"ano\", mode=\"OverWrite\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Table or view not found: relatorioanualparticionado;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1017.table.\n: org.apache.spark.sql.AnalysisException: Table or view not found: relatorioanualparticionado;\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:47)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:733)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:685)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:715)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:708)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:708)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:654)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.SparkSession.table(SparkSession.scala:628)\n\tat org.apache.spark.sql.SparkSession.table(SparkSession.scala:624)\n\tat org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:673)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'relatorioanualparticionado' not found in database 'default';\n\tat org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)\n\tat org.apache.spark.sql.hive.client.HiveClient$$anonfun$getTable$1.apply(HiveClient.scala:81)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.sql.hive.client.HiveClient$class.getTable(HiveClient.scala:81)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.getTable(HiveClientImpl.scala:83)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getRawTable(HiveExternalCatalog.scala:118)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:700)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$getTable$1.apply(HiveExternalCatalog.scala:700)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.getTable(HiveExternalCatalog.scala:699)\n\tat org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTable(ExternalCatalogWithListener.scala:138)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:706)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:730)\n\t... 45 more\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-5bace02b0d23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"relatorioanualparticionado\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mtable\u001b[0;34m(self, tableName)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'day'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \"\"\"\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtableName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Table or view not found: relatorioanualparticionado;'"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"relatorioanualparticionado\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
